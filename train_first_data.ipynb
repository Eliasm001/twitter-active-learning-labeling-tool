{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier entwickeln wir das Modell (bzw. testen es aus) -> bevor man es in die richtige Codebase macht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notwendige Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "#from sklearn.metrics import accuracy\n",
    "#import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing -> TODO multiclass prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/twitter_sentiment_data.csv\")\n",
    "df.loc[df.sentiment == -1, \"sentiment\"] = 2\n",
    "df = df[df.sentiment < 2]\n",
    "x_train,x_test,y_train,y_test = train_test_split(df.message.values,df.sentiment.values,test_size= 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelle -> Ergänzungen hier drunter einfach als tokenizer_modelname bzw. model_modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = transformers.TFAutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiny bert hat bei mir 7 min 20 gebrauch (für 2/3 des gesamten Kaggle Datensatzes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "encoded_x_train = tokenizer(list(x_train),padding=True, return_tensors=\"np\")\n",
    "model.compile()\n",
    "model.fit(encoded_x_train.data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 119s 376ms/step\n"
     ]
    }
   ],
   "source": [
    "encoded_x_test = tokenizer(list(x_test),padding=True, return_tensors=\"np\")\n",
    "y_pred = model.predict(encoded_x_test.data)\n",
    "pred_labels = np.argmax(y_pred.logits, axis=1)\n",
    "cert = np.max(y_pred.logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8048202291584354"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(pred_labels,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Teamprojekt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae1f75188e45fa9dc73ac348c85823199c2f375d787fc458e757a858a1308914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
